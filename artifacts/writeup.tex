\documentclass[twocolumn]{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}

\title{Lab 0: Sorting Algorithm Optimization}
\author{Team 22}
\date{September 2024}

\begin{document}

\maketitle

\section{Overview}
This lab focuses on optimizing sorting algorithms for performance, a crucial skill in computer science. We'll start with unoptimized baseline implementations and iteratively create multiple variants to improve efficiency. The main objectives include evaluating different sorting algorithms (with at least two recursive sorts), fine-tuning a specific sort, implementing a dispatch routine to select the optimal sort based on problem size, incorporating dispatch into a recursive sort, and demonstrating low-level optimizations such as instruction-level parallelism. This lab emphasizes hands-on experience in performance optimization, encouraging us to test our hypotheses and refine our approach through multiple iterations. It's an excellent opportunity to apply theoretical knowledge from our coursework to practical problems, bridging the gap between classroom learning and real-world software optimization challenges. The competitive aspect, with bonus points for top performers, adds an exciting dimension to the project and motivates us to push our optimization skills to the limit.

\section{Course Relevance}
This lab assignment integrates concepts from several courses in our computer science curriculum. Our \textit{Data Structures and Algorithms} course laid the foundation for understanding various sorting algorithms and their complexity, which is crucial for this optimization task. \textit{Computer Architecture} provides insights into hardware-level optimizations, especially relevant for tasks involving instruction-level parallelism and cache utilization. The \textit{Operating Systems} course helps us understand process management and memory hierarchies, essential for efficient sorting implementations. Our \textit{Programming Languages} course aids in writing efficient code across different paradigms, including recursive implementations. The \textit{Software Engineering} course's emphasis on iterative development and performance analysis aligns perfectly with the lab's approach to creating and evaluating multiple variants of sorting algorithms. Lastly, our \textit{Analysis of Algorithms} course provides the theoretical backdrop for understanding time complexity and space-time tradeoffs, which is vital when comparing and optimizing different sorting techniques.

\section{Refinements}

\subsection{Variant 1: Bubble Sort}

\subsubsection{Changes Made}
In this variant, we implemented an optimized version of the given Bubblesort. The main change is.:
\begin{itemize}
    \item Checking if a swap has happened so it does less operations
\end{itemize}

\subsubsection{Expected Results}
We expect to see improved performance, We reduce the number of steps  the algorithm takes

\subsubsection{Performance Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\columnwidth]{var0.png}
    \caption{Performance plot for Optimized Bubblesort}
    \label{fig:variant1_plot}
\end{figure}

The performance plot shows  improvement in execution time compared to the baseline.
\begin{itemize}
    \item For small input sizes it is significantly quicker.
    \item For larger input sizes it is the same as the baseline
\end{itemize}

\subsubsection{Potential Improvements}
To further enhance this variant, we could:
\begin{itemize}
    \item  Unroll the loop

\end{itemize}

\subsection{Variant 2: Quicksort}

\subsubsection{Changes Made}
This started as a very basic implementation of Quicksort, but these potential improvements were made:
\begin{itemize}
    \item Implemented Hoare partitioning scheme, but was actually slower. This is probably due to the fact that the data we sorted was completely random and so did not benefit from the new partitioning.
    \item Changed around some if statements to help branch predictor.
    \item Implement multithreaded quicksort.
\end{itemize}

\subsubsection{Expected Results}
It was expected that the Hoare partition to be faster since it looked like it would do less compares and swaps. As well, the threaded implementation is going to be slower at smaller sizes due to thread spawning overhead.

\subsubsection{Performance Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\columnwidth]{var1.png}
    \caption{Performance plot for optimized Quicksort}
    \label{fig:variant2_plot}
\end{figure}

We can see that there is not too much difference between the first few optimizations.
\begin{itemize}
    \item Unfortunately, the Hoare partitioning is not actually faster, it seems like it is only better when the data is not completely random.
    \item The change to comparisons did not seem to have a major effect, we might have expected this since the compiler is probably best at generating fewer branches.
	\item The threaded implementation starts out worse because of overhead but slowly get better, I expect for very large sizes it is the best.
\end{itemize}

\subsubsection{Potential Improvements}
To enhance this variant further, we could:
\begin{itemize}
    \item Use a threadpool to offload all thread generating overhead to the start.
    \item Add some better load balancing so that threads aren't spawned when the partition is very uneven.
\end{itemize}
\subsection{Variant 3: Selection Sort }

\subsubsection{Changes Made}
In this variant, we optimized the cloning of the arrays and implemented selection sort
\begin{itemize}
    \item The baseline code had a loop copying mechanism before the sorting even started. It first copies the items from list x to y and then performs sorting. We removed this initial loop copying of the list and modified the sorting to sort on the input array and copy the result in the put array while the sorting loop was processing. 
    \item We tried to use OpenMP directives for the inner loop parallelization by dividing the search for minimum element across multiple threads. We also added a critical section to make sure only one thread updates the minimum value.
    \item  This was done on top of the parallelism part where we removed the if conditionals for the minimum value check. We wanted to not branch execution due to the if statement.
\end{itemize}

\subsubsection{Expected Results}
We expect to see improved performance, especially for small datasets, and an overall improvement from the base bubblesort.
\subsubsection{Performance Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\columnwidth]{var2.png}
    \caption{Performance plot for Optimized Selectionsort}
    \label{fig:variant3_plot}
\end{figure}

The performance plot shows improvement in execution time compared to the baseline bubblesort  implementation. We observe that:
\begin{itemize}
    \item For small input sizes a speedup in time
    \item Parallelizing for small sizes is not effective
\end{itemize}

\subsubsection{Potential Improvements}
To further enhance this variant, we could:
\begin{itemize}
    \item Parallelize the outer loop
    \item Unroll loops
    \item Improve memory access.
\end{itemize}

\subsection{Variant 4: Merge sort:}

Merge sort is an algorithm with the divide-and-conquer approach. it works by recursively dividing the input, sorting the smaller sections and merging them together.

\subsubsection{Changes Made}
In this variant, we implemented a optimized version of metgesort
\begin{itemize}
    \item Var300 : Created mergesort base algorythm
    \item Var301: Optimized mergsort by adding insertion sort for small arrays.
    \item Var 302: Tried to optimize mergesort by converting to blocksrot with size $\sqrt(n)$
    \item Var 303: Made the 302 implementation of mergesort into a parallel version.
\end{itemize}

\subsubsection{Expected Results}
\begin{itemize}
	\item Var300: We expected an improvement from the base sort
	\item Var301: We expected an improvement of speed for small sizes
	\item Var302: We expected an improvement for large arrays
	\item Var303: We expected an improvement for very large arrays compared to the previous variants.
\end{itemize}

\subsubsection{Performance Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\columnwidth]{var3.png}
    \caption{Performance plot for Optimized Mergesort}
    \label{fig:variant4_plot}
\end{figure}

The performance plot shows a significant improvement in execution time compared to the baseline  implementation. We observe that:
\begin{itemize}
    \item For small input sizes the optimized version performs slightly better due to the insertion sort optimization.
    \item For larger inputs, we see a more pronounced improvement, with the optimized version consistently outperforming the baseline.
    \item The performance gain increases with input size, suggesting that our optimizations are particularly effective for larger datasets.
\end{itemize}

\subsubsection{Potential Improvements}
To further enhance this variant, we could:
\begin{itemize}
    \item Experiment with different thresholds for initializing new threads.
    \item Implement a parallel version of the blocksort approach.
    \item Reduce the work made by the for loops.
    \item Addign all memory before recursion and threading starts.
\end{itemize}

\section{Master}

After analyzing all of the variants and comparing the throughputs we combined the best and assigned them to their most optimal sizes.

\subsubsection{Changes Made}
\begin{itemize}
	\item For small input size ($n<32$) use Insertionsort
	\item For medium input size ($n<40000$) use Quicksort
	\item For large input size ($n > 40000$) use Parallel Quicksort.
\end{itemize}

\subsubsection{Performance Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\columnwidth]{var_master.png}
    \caption{Performance plot for best algorithm}
    \label{fig:variantmaster_plot}
\end{figure}

The performance plot shows better performance at all problem sizes than any one algorithm. We observe that:
\begin{itemize}
    \item For small input sizes insertion sort is very good.
    \item For larger input sizes quicksort is satisfactory.
    \item For very large input sizes, when the size counteracts the overhead of starting threads, throughput can increase.
\end{itemize}


\section{Conclusion}
With this lab we've learned how to benchmark and improve our algorithms iteratively and to take the best out of each. With our master sort, we use the correct tool for the correct problem size, allowing for optimal performance in all cases. At small sizes we use insertion sort with it's low overhead, for large sizes we use quicksort with it's divide-and-conquer strategy, and for the largest sizes we use parallel quicksort to take advantage of modern cpu's


\end{document}
